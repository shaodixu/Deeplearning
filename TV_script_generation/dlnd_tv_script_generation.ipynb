{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    counts = Counter(text)\n",
    "    counts = sorted(counts,key=counts.get,reverse=True)\n",
    "    vocab_to_int = {word:ii for ii, word in enumerate(counts,1)}\n",
    "    int_to_vocab = {ii:word for ii, word in enumerate(counts,1)}\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "  #  token_specification = {'Period': '.',\n",
    "   # 'Comma': ',', \n",
    "    #'Quotation Mark': '\"',\n",
    "    #'Semicolon': ';', \n",
    "    #'Exclamation mark': '!', \n",
    "    #'Question Mark': '?', \n",
    "    #'Left Parantheses': '(', \n",
    "    #'Right Parentheses': ')', \n",
    "    #'Dash': '--',\n",
    "    #'Return': '\\n'}\n",
    "    #return token_specification\n",
    "\n",
    "    \n",
    "    return {'.': '||Period||',\n",
    "            ',': '||Comma||',\n",
    "            '?': '||Question_Mark||',\n",
    "            ';': '||Semicolon||',\n",
    "            '!': '||Exclamation_Mark||',\n",
    "           \n",
    "            '(': '||Left_Parentheses||',\n",
    "            ')': '||Right_Parentheses||',\n",
    "            '--': '||Dash||',\n",
    "            '\\n': '||Return||',\n",
    "            '\"': '||Quotation_Mark||'\n",
    "            \n",
    "            \n",
    "        }\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = tf.placeholder(tf.int32, [None, None],name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = 'targets')\n",
    "    LearningRate = tf.placeholder(tf.float32)\n",
    "    return (inputs, targets, LearningRate)\n",
    "\n",
    "\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#    number_of_layers = 2\n",
    "    \n",
    "#    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "#    cell = tf.contrib.rnn.MultiRNNCell([cell]*number_of_layers)\n",
    "    \n",
    "#    state = cell.zero_state(batch_size, tf.float32)\n",
    "#    InitialState = tf.identity(state, 'initial_state')\n",
    "    \n",
    "#    return (cell, InitialState)\n",
    "\n",
    "\n",
    "    number_of_layers = 1\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell]*number_of_layers )\n",
    "   # batch_size = tf.placeholder(tf.int32,[])\n",
    "\n",
    "    X = cell.zero_state(batch_size, tf.float32)\n",
    "    init_state = tf.identity(X, name='initial_state')\n",
    "    return cell, init_state\n",
    "\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#    embedding = tf.Variable(tf.random_uniform([vocab_size,embed_dim],-1,1))\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1,1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    \n",
    "    \n",
    "    return embed\n",
    "\n",
    "\n",
    "\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell,inputs,dtype='float32')\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    return (outputs, final_state)\n",
    "\n",
    "\n",
    "\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell,embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size,\n",
    "                                               weights_initializer = tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                               biases_initializer=tf.zeros_initializer(),\n",
    "                                               activation_fn=None)\n",
    "    return (logits, final_state)\n",
    "\n",
    "\n",
    "\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    n_batches = len(int_text)//(batch_size*seq_length)\n",
    "    inputs = np.array(int_text[:n_batches*(batch_size*seq_length)])\n",
    "    targets = np.array(int_text[1:n_batches*(batch_size*seq_length)+1])\n",
    "    targets[-1] = inputs[0]\n",
    "    input_batches = np.split(inputs.reshape(batch_size, -1), n_batches, 1)\n",
    "    target_batches = np.split(targets.reshape(batch_size, -1), n_batches, 1)\n",
    "\n",
    "    output = np.array(list(zip(input_batches, target_batches)))\n",
    "    #output = output.reshape(num_batches, 2, batch_size, seq_length)\n",
    "    return output\n",
    "\n",
    "\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 1000\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "embed_dim =  100\n",
    "# Sequence Length\n",
    "seq_length = 12\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/44   train_loss = 8.831\n",
      "Epoch   2 Batch   12/44   train_loss = 5.374\n",
      "Epoch   4 Batch   24/44   train_loss = 6.233\n",
      "Epoch   6 Batch   36/44   train_loss = 6.983\n",
      "Epoch   9 Batch    4/44   train_loss = 6.721\n",
      "Epoch  11 Batch   16/44   train_loss = 6.976\n",
      "Epoch  13 Batch   28/44   train_loss = 6.760\n",
      "Epoch  15 Batch   40/44   train_loss = 7.008\n",
      "Epoch  18 Batch    8/44   train_loss = 7.238\n",
      "Epoch  20 Batch   20/44   train_loss = 6.760\n",
      "Epoch  22 Batch   32/44   train_loss = 6.877\n",
      "Epoch  25 Batch    0/44   train_loss = 6.761\n",
      "Epoch  27 Batch   12/44   train_loss = 6.818\n",
      "Epoch  29 Batch   24/44   train_loss = 7.225\n",
      "Epoch  31 Batch   36/44   train_loss = 7.262\n",
      "Epoch  34 Batch    4/44   train_loss = 7.193\n",
      "Epoch  36 Batch   16/44   train_loss = 7.187\n",
      "Epoch  38 Batch   28/44   train_loss = 7.131\n",
      "Epoch  40 Batch   40/44   train_loss = 6.964\n",
      "Epoch  43 Batch    8/44   train_loss = 7.009\n",
      "Epoch  45 Batch   20/44   train_loss = 6.819\n",
      "Epoch  47 Batch   32/44   train_loss = 6.818\n",
      "Epoch  50 Batch    0/44   train_loss = 6.719\n",
      "Epoch  52 Batch   12/44   train_loss = 6.741\n",
      "Epoch  54 Batch   24/44   train_loss = 6.723\n",
      "Epoch  56 Batch   36/44   train_loss = 6.848\n",
      "Epoch  59 Batch    4/44   train_loss = 6.763\n",
      "Epoch  61 Batch   16/44   train_loss = 6.821\n",
      "Epoch  63 Batch   28/44   train_loss = 6.794\n",
      "Epoch  65 Batch   40/44   train_loss = 6.693\n",
      "Epoch  68 Batch    8/44   train_loss = 6.718\n",
      "Epoch  70 Batch   20/44   train_loss = 6.577\n",
      "Epoch  72 Batch   32/44   train_loss = 6.566\n",
      "Epoch  75 Batch    0/44   train_loss = 6.516\n",
      "Epoch  77 Batch   12/44   train_loss = 6.556\n",
      "Epoch  79 Batch   24/44   train_loss = 6.553\n",
      "Epoch  81 Batch   36/44   train_loss = 6.677\n",
      "Epoch  84 Batch    4/44   train_loss = 6.596\n",
      "Epoch  86 Batch   16/44   train_loss = 6.683\n",
      "Epoch  88 Batch   28/44   train_loss = 6.645\n",
      "Epoch  90 Batch   40/44   train_loss = 6.567\n",
      "Epoch  93 Batch    8/44   train_loss = 6.556\n",
      "Epoch  95 Batch   20/44   train_loss = 6.456\n",
      "Epoch  97 Batch   32/44   train_loss = 6.446\n",
      "Epoch 100 Batch    0/44   train_loss = 6.376\n",
      "Epoch 102 Batch   12/44   train_loss = 6.417\n",
      "Epoch 104 Batch   24/44   train_loss = 6.424\n",
      "Epoch 106 Batch   36/44   train_loss = 6.554\n",
      "Epoch 109 Batch    4/44   train_loss = 6.453\n",
      "Epoch 111 Batch   16/44   train_loss = 6.557\n",
      "Epoch 113 Batch   28/44   train_loss = 6.520\n",
      "Epoch 115 Batch   40/44   train_loss = 6.454\n",
      "Epoch 118 Batch    8/44   train_loss = 6.428\n",
      "Epoch 120 Batch   20/44   train_loss = 6.351\n",
      "Epoch 122 Batch   32/44   train_loss = 6.344\n",
      "Epoch 125 Batch    0/44   train_loss = 6.272\n",
      "Epoch 127 Batch   12/44   train_loss = 6.315\n",
      "Epoch 129 Batch   24/44   train_loss = 6.329\n",
      "Epoch 131 Batch   36/44   train_loss = 6.463\n",
      "Epoch 134 Batch    4/44   train_loss = 6.347\n",
      "Epoch 136 Batch   16/44   train_loss = 6.464\n",
      "Epoch 138 Batch   28/44   train_loss = 6.427\n",
      "Epoch 140 Batch   40/44   train_loss = 6.370\n",
      "Epoch 143 Batch    8/44   train_loss = 6.334\n",
      "Epoch 145 Batch   20/44   train_loss = 6.274\n",
      "Epoch 147 Batch   32/44   train_loss = 6.271\n",
      "Epoch 150 Batch    0/44   train_loss = 6.195\n",
      "Epoch 152 Batch   12/44   train_loss = 6.241\n",
      "Epoch 154 Batch   24/44   train_loss = 6.262\n",
      "Epoch 156 Batch   36/44   train_loss = 6.397\n",
      "Epoch 159 Batch    4/44   train_loss = 6.270\n",
      "Epoch 161 Batch   16/44   train_loss = 6.399\n",
      "Epoch 163 Batch   28/44   train_loss = 6.361\n",
      "Epoch 165 Batch   40/44   train_loss = 6.311\n",
      "Epoch 168 Batch    8/44   train_loss = 6.268\n",
      "Epoch 170 Batch   20/44   train_loss = 6.219\n",
      "Epoch 172 Batch   32/44   train_loss = 6.220\n",
      "Epoch 175 Batch    0/44   train_loss = 6.141\n",
      "Epoch 177 Batch   12/44   train_loss = 6.189\n",
      "Epoch 179 Batch   24/44   train_loss = 6.216\n",
      "Epoch 181 Batch   36/44   train_loss = 6.350\n",
      "Epoch 184 Batch    4/44   train_loss = 6.216\n",
      "Epoch 186 Batch   16/44   train_loss = 6.352\n",
      "Epoch 188 Batch   28/44   train_loss = 6.314\n",
      "Epoch 190 Batch   40/44   train_loss = 6.269\n",
      "Epoch 193 Batch    8/44   train_loss = 6.221\n",
      "Epoch 195 Batch   20/44   train_loss = 6.179\n",
      "Epoch 197 Batch   32/44   train_loss = 6.183\n",
      "Epoch 200 Batch    0/44   train_loss = 6.101\n",
      "Epoch 202 Batch   12/44   train_loss = 6.151\n",
      "Epoch 204 Batch   24/44   train_loss = 6.183\n",
      "Epoch 206 Batch   36/44   train_loss = 6.314\n",
      "Epoch 209 Batch    4/44   train_loss = 6.176\n",
      "Epoch 211 Batch   16/44   train_loss = 6.317\n",
      "Epoch 213 Batch   28/44   train_loss = 6.279\n",
      "Epoch 215 Batch   40/44   train_loss = 6.237\n",
      "Epoch 218 Batch    8/44   train_loss = 6.187\n",
      "Epoch 220 Batch   20/44   train_loss = 6.149\n",
      "Epoch 222 Batch   32/44   train_loss = 6.155\n",
      "Epoch 225 Batch    0/44   train_loss = 6.070\n",
      "Epoch 227 Batch   12/44   train_loss = 6.121\n",
      "Epoch 229 Batch   24/44   train_loss = 6.156\n",
      "Epoch 231 Batch   36/44   train_loss = 6.285\n",
      "Epoch 234 Batch    4/44   train_loss = 6.144\n",
      "Epoch 236 Batch   16/44   train_loss = 6.289\n",
      "Epoch 238 Batch   28/44   train_loss = 6.250\n",
      "Epoch 240 Batch   40/44   train_loss = 6.211\n",
      "Epoch 243 Batch    8/44   train_loss = 6.160\n",
      "Epoch 245 Batch   20/44   train_loss = 6.124\n",
      "Epoch 247 Batch   32/44   train_loss = 6.130\n",
      "Epoch 250 Batch    0/44   train_loss = 6.044\n",
      "Epoch 252 Batch   12/44   train_loss = 6.096\n",
      "Epoch 254 Batch   24/44   train_loss = 6.134\n",
      "Epoch 256 Batch   36/44   train_loss = 6.260\n",
      "Epoch 259 Batch    4/44   train_loss = 6.117\n",
      "Epoch 261 Batch   16/44   train_loss = 6.264\n",
      "Epoch 263 Batch   28/44   train_loss = 6.226\n",
      "Epoch 265 Batch   40/44   train_loss = 6.188\n",
      "Epoch 268 Batch    8/44   train_loss = 6.137\n",
      "Epoch 270 Batch   20/44   train_loss = 6.102\n",
      "Epoch 272 Batch   32/44   train_loss = 6.109\n",
      "Epoch 275 Batch    0/44   train_loss = 6.022\n",
      "Epoch 277 Batch   12/44   train_loss = 6.074\n",
      "Epoch 279 Batch   24/44   train_loss = 6.114\n",
      "Epoch 281 Batch   36/44   train_loss = 6.237\n",
      "Epoch 284 Batch    4/44   train_loss = 6.093\n",
      "Epoch 286 Batch   16/44   train_loss = 6.241\n",
      "Epoch 288 Batch   28/44   train_loss = 6.204\n",
      "Epoch 290 Batch   40/44   train_loss = 6.167\n",
      "Epoch 293 Batch    8/44   train_loss = 6.117\n",
      "Epoch 295 Batch   20/44   train_loss = 6.082\n",
      "Epoch 297 Batch   32/44   train_loss = 6.089\n",
      "Epoch 300 Batch    0/44   train_loss = 6.001\n",
      "Epoch 302 Batch   12/44   train_loss = 6.054\n",
      "Epoch 304 Batch   24/44   train_loss = 6.095\n",
      "Epoch 306 Batch   36/44   train_loss = 6.216\n",
      "Epoch 309 Batch    4/44   train_loss = 6.073\n",
      "Epoch 311 Batch   16/44   train_loss = 6.221\n",
      "Epoch 313 Batch   28/44   train_loss = 6.184\n",
      "Epoch 315 Batch   40/44   train_loss = 6.148\n",
      "Epoch 318 Batch    8/44   train_loss = 6.099\n",
      "Epoch 320 Batch   20/44   train_loss = 6.064\n",
      "Epoch 322 Batch   32/44   train_loss = 6.071\n",
      "Epoch 325 Batch    0/44   train_loss = 5.983\n",
      "Epoch 327 Batch   12/44   train_loss = 6.037\n",
      "Epoch 329 Batch   24/44   train_loss = 6.079\n",
      "Epoch 331 Batch   36/44   train_loss = 6.197\n",
      "Epoch 334 Batch    4/44   train_loss = 6.054\n",
      "Epoch 336 Batch   16/44   train_loss = 6.202\n",
      "Epoch 338 Batch   28/44   train_loss = 6.166\n",
      "Epoch 340 Batch   40/44   train_loss = 6.131\n",
      "Epoch 343 Batch    8/44   train_loss = 6.082\n",
      "Epoch 345 Batch   20/44   train_loss = 6.047\n",
      "Epoch 347 Batch   32/44   train_loss = 6.055\n",
      "Epoch 350 Batch    0/44   train_loss = 5.966\n",
      "Epoch 352 Batch   12/44   train_loss = 6.020\n",
      "Epoch 354 Batch   24/44   train_loss = 6.064\n",
      "Epoch 356 Batch   36/44   train_loss = 6.179\n",
      "Epoch 359 Batch    4/44   train_loss = 6.038\n",
      "Epoch 361 Batch   16/44   train_loss = 6.185\n",
      "Epoch 363 Batch   28/44   train_loss = 6.150\n",
      "Epoch 365 Batch   40/44   train_loss = 6.115\n",
      "Epoch 368 Batch    8/44   train_loss = 6.068\n",
      "Epoch 370 Batch   20/44   train_loss = 6.033\n",
      "Epoch 372 Batch   32/44   train_loss = 6.040\n",
      "Epoch 375 Batch    0/44   train_loss = 5.951\n",
      "Epoch 377 Batch   12/44   train_loss = 6.006\n",
      "Epoch 379 Batch   24/44   train_loss = 6.050\n",
      "Epoch 381 Batch   36/44   train_loss = 6.163\n",
      "Epoch 384 Batch    4/44   train_loss = 6.023\n",
      "Epoch 386 Batch   16/44   train_loss = 6.170\n",
      "Epoch 388 Batch   28/44   train_loss = 6.135\n",
      "Epoch 390 Batch   40/44   train_loss = 6.101\n",
      "Epoch 393 Batch    8/44   train_loss = 6.054\n",
      "Epoch 395 Batch   20/44   train_loss = 6.019\n",
      "Epoch 397 Batch   32/44   train_loss = 6.026\n",
      "Epoch 400 Batch    0/44   train_loss = 5.937\n",
      "Epoch 402 Batch   12/44   train_loss = 5.992\n",
      "Epoch 404 Batch   24/44   train_loss = 6.037\n",
      "Epoch 406 Batch   36/44   train_loss = 6.148\n",
      "Epoch 409 Batch    4/44   train_loss = 6.010\n",
      "Epoch 411 Batch   16/44   train_loss = 6.155\n",
      "Epoch 413 Batch   28/44   train_loss = 6.122\n",
      "Epoch 415 Batch   40/44   train_loss = 6.087\n",
      "Epoch 418 Batch    8/44   train_loss = 6.042\n",
      "Epoch 420 Batch   20/44   train_loss = 6.007\n",
      "Epoch 422 Batch   32/44   train_loss = 6.013\n",
      "Epoch 425 Batch    0/44   train_loss = 5.924\n",
      "Epoch 427 Batch   12/44   train_loss = 5.980\n",
      "Epoch 429 Batch   24/44   train_loss = 6.025\n",
      "Epoch 431 Batch   36/44   train_loss = 6.135\n",
      "Epoch 434 Batch    4/44   train_loss = 5.998\n",
      "Epoch 436 Batch   16/44   train_loss = 6.142\n",
      "Epoch 438 Batch   28/44   train_loss = 6.110\n",
      "Epoch 440 Batch   40/44   train_loss = 6.075\n",
      "Epoch 443 Batch    8/44   train_loss = 6.030\n",
      "Epoch 445 Batch   20/44   train_loss = 5.995\n",
      "Epoch 447 Batch   32/44   train_loss = 6.001\n",
      "Epoch 450 Batch    0/44   train_loss = 5.912\n",
      "Epoch 452 Batch   12/44   train_loss = 5.969\n",
      "Epoch 454 Batch   24/44   train_loss = 6.014\n",
      "Epoch 456 Batch   36/44   train_loss = 6.123\n",
      "Epoch 459 Batch    4/44   train_loss = 5.987\n",
      "Epoch 461 Batch   16/44   train_loss = 6.130\n",
      "Epoch 463 Batch   28/44   train_loss = 6.099\n",
      "Epoch 465 Batch   40/44   train_loss = 6.064\n",
      "Epoch 468 Batch    8/44   train_loss = 6.020\n",
      "Epoch 470 Batch   20/44   train_loss = 5.985\n",
      "Epoch 472 Batch   32/44   train_loss = 5.990\n",
      "Epoch 475 Batch    0/44   train_loss = 5.902\n",
      "Epoch 477 Batch   12/44   train_loss = 5.959\n",
      "Epoch 479 Batch   24/44   train_loss = 6.005\n",
      "Epoch 481 Batch   36/44   train_loss = 6.111\n",
      "Epoch 484 Batch    4/44   train_loss = 5.977\n",
      "Epoch 486 Batch   16/44   train_loss = 6.119\n",
      "Epoch 488 Batch   28/44   train_loss = 6.089\n",
      "Epoch 490 Batch   40/44   train_loss = 6.053\n",
      "Epoch 493 Batch    8/44   train_loss = 6.010\n",
      "Epoch 495 Batch   20/44   train_loss = 5.975\n",
      "Epoch 497 Batch   32/44   train_loss = 5.981\n",
      "Epoch 500 Batch    0/44   train_loss = 5.892\n",
      "Epoch 502 Batch   12/44   train_loss = 5.949\n",
      "Epoch 504 Batch   24/44   train_loss = 5.996\n",
      "Epoch 506 Batch   36/44   train_loss = 6.100\n",
      "Epoch 509 Batch    4/44   train_loss = 5.969\n",
      "Epoch 511 Batch   16/44   train_loss = 6.109\n",
      "Epoch 513 Batch   28/44   train_loss = 6.080\n",
      "Epoch 515 Batch   40/44   train_loss = 6.041\n",
      "Epoch 518 Batch    8/44   train_loss = 6.002\n",
      "Epoch 520 Batch   20/44   train_loss = 5.968\n",
      "Epoch 522 Batch   32/44   train_loss = 5.973\n",
      "Epoch 525 Batch    0/44   train_loss = 5.883\n",
      "Epoch 527 Batch   12/44   train_loss = 5.940\n",
      "Epoch 529 Batch   24/44   train_loss = 5.987\n",
      "Epoch 531 Batch   36/44   train_loss = 6.089\n",
      "Epoch 534 Batch    4/44   train_loss = 5.962\n",
      "Epoch 536 Batch   16/44   train_loss = 6.099\n",
      "Epoch 538 Batch   28/44   train_loss = 6.071\n",
      "Epoch 540 Batch   40/44   train_loss = 6.031\n",
      "Epoch 543 Batch    8/44   train_loss = 5.994\n",
      "Epoch 545 Batch   20/44   train_loss = 5.960\n",
      "Epoch 547 Batch   32/44   train_loss = 5.965\n",
      "Epoch 550 Batch    0/44   train_loss = 5.874\n",
      "Epoch 552 Batch   12/44   train_loss = 5.932\n",
      "Epoch 554 Batch   24/44   train_loss = 5.979\n",
      "Epoch 556 Batch   36/44   train_loss = 6.080\n",
      "Epoch 559 Batch    4/44   train_loss = 5.955\n",
      "Epoch 561 Batch   16/44   train_loss = 6.091\n",
      "Epoch 563 Batch   28/44   train_loss = 6.064\n",
      "Epoch 565 Batch   40/44   train_loss = 6.021\n",
      "Epoch 568 Batch    8/44   train_loss = 5.987\n",
      "Epoch 570 Batch   20/44   train_loss = 5.953\n",
      "Epoch 572 Batch   32/44   train_loss = 5.958\n",
      "Epoch 575 Batch    0/44   train_loss = 5.866\n",
      "Epoch 577 Batch   12/44   train_loss = 5.924\n",
      "Epoch 579 Batch   24/44   train_loss = 5.972\n",
      "Epoch 581 Batch   36/44   train_loss = 6.071\n",
      "Epoch 584 Batch    4/44   train_loss = 5.949\n",
      "Epoch 586 Batch   16/44   train_loss = 6.083\n",
      "Epoch 588 Batch   28/44   train_loss = 6.057\n",
      "Epoch 590 Batch   40/44   train_loss = 6.012\n",
      "Epoch 593 Batch    8/44   train_loss = 5.980\n",
      "Epoch 595 Batch   20/44   train_loss = 5.947\n",
      "Epoch 597 Batch   32/44   train_loss = 5.951\n",
      "Epoch 600 Batch    0/44   train_loss = 5.859\n",
      "Epoch 602 Batch   12/44   train_loss = 5.917\n",
      "Epoch 604 Batch   24/44   train_loss = 5.965\n",
      "Epoch 606 Batch   36/44   train_loss = 6.063\n",
      "Epoch 609 Batch    4/44   train_loss = 5.943\n",
      "Epoch 611 Batch   16/44   train_loss = 6.075\n",
      "Epoch 613 Batch   28/44   train_loss = 6.051\n",
      "Epoch 615 Batch   40/44   train_loss = 6.004\n",
      "Epoch 618 Batch    8/44   train_loss = 5.974\n",
      "Epoch 620 Batch   20/44   train_loss = 5.941\n",
      "Epoch 622 Batch   32/44   train_loss = 5.945\n",
      "Epoch 625 Batch    0/44   train_loss = 5.852\n",
      "Epoch 627 Batch   12/44   train_loss = 5.911\n",
      "Epoch 629 Batch   24/44   train_loss = 5.959\n",
      "Epoch 631 Batch   36/44   train_loss = 6.056\n",
      "Epoch 634 Batch    4/44   train_loss = 5.937\n",
      "Epoch 636 Batch   16/44   train_loss = 6.068\n",
      "Epoch 638 Batch   28/44   train_loss = 6.045\n",
      "Epoch 640 Batch   40/44   train_loss = 5.997\n",
      "Epoch 643 Batch    8/44   train_loss = 5.968\n",
      "Epoch 645 Batch   20/44   train_loss = 5.935\n",
      "Epoch 647 Batch   32/44   train_loss = 5.938\n",
      "Epoch 650 Batch    0/44   train_loss = 5.846\n",
      "Epoch 652 Batch   12/44   train_loss = 5.905\n",
      "Epoch 654 Batch   24/44   train_loss = 5.953\n",
      "Epoch 656 Batch   36/44   train_loss = 6.049\n",
      "Epoch 659 Batch    4/44   train_loss = 5.932\n",
      "Epoch 661 Batch   16/44   train_loss = 6.062\n",
      "Epoch 663 Batch   28/44   train_loss = 6.039\n",
      "Epoch 665 Batch   40/44   train_loss = 5.990\n",
      "Epoch 668 Batch    8/44   train_loss = 5.962\n",
      "Epoch 670 Batch   20/44   train_loss = 5.929\n",
      "Epoch 672 Batch   32/44   train_loss = 5.933\n",
      "Epoch 675 Batch    0/44   train_loss = 5.840\n",
      "Epoch 677 Batch   12/44   train_loss = 5.899\n",
      "Epoch 679 Batch   24/44   train_loss = 5.947\n",
      "Epoch 681 Batch   36/44   train_loss = 6.042\n",
      "Epoch 684 Batch    4/44   train_loss = 5.927\n",
      "Epoch 686 Batch   16/44   train_loss = 6.055\n",
      "Epoch 688 Batch   28/44   train_loss = 6.034\n",
      "Epoch 690 Batch   40/44   train_loss = 5.983\n",
      "Epoch 693 Batch    8/44   train_loss = 5.957\n",
      "Epoch 695 Batch   20/44   train_loss = 5.924\n",
      "Epoch 697 Batch   32/44   train_loss = 5.927\n",
      "Epoch 700 Batch    0/44   train_loss = 5.834\n",
      "Epoch 702 Batch   12/44   train_loss = 5.894\n",
      "Epoch 704 Batch   24/44   train_loss = 5.942\n",
      "Epoch 706 Batch   36/44   train_loss = 6.036\n",
      "Epoch 709 Batch    4/44   train_loss = 5.922\n",
      "Epoch 711 Batch   16/44   train_loss = 6.050\n",
      "Epoch 713 Batch   28/44   train_loss = 6.029\n",
      "Epoch 715 Batch   40/44   train_loss = 5.977\n",
      "Epoch 718 Batch    8/44   train_loss = 5.952\n",
      "Epoch 720 Batch   20/44   train_loss = 5.919\n",
      "Epoch 722 Batch   32/44   train_loss = 5.922\n",
      "Epoch 725 Batch    0/44   train_loss = 5.829\n",
      "Epoch 727 Batch   12/44   train_loss = 5.889\n",
      "Epoch 729 Batch   24/44   train_loss = 5.937\n",
      "Epoch 731 Batch   36/44   train_loss = 6.030\n",
      "Epoch 734 Batch    4/44   train_loss = 5.918\n",
      "Epoch 736 Batch   16/44   train_loss = 6.044\n",
      "Epoch 738 Batch   28/44   train_loss = 6.024\n",
      "Epoch 740 Batch   40/44   train_loss = 5.972\n",
      "Epoch 743 Batch    8/44   train_loss = 5.947\n",
      "Epoch 745 Batch   20/44   train_loss = 5.915\n",
      "Epoch 747 Batch   32/44   train_loss = 5.917\n",
      "Epoch 750 Batch    0/44   train_loss = 5.824\n",
      "Epoch 752 Batch   12/44   train_loss = 5.884\n",
      "Epoch 754 Batch   24/44   train_loss = 5.932\n",
      "Epoch 756 Batch   36/44   train_loss = 6.025\n",
      "Epoch 759 Batch    4/44   train_loss = 5.913\n",
      "Epoch 761 Batch   16/44   train_loss = 6.039\n",
      "Epoch 763 Batch   28/44   train_loss = 6.020\n",
      "Epoch 765 Batch   40/44   train_loss = 5.966\n",
      "Epoch 768 Batch    8/44   train_loss = 5.942\n",
      "Epoch 770 Batch   20/44   train_loss = 5.910\n",
      "Epoch 772 Batch   32/44   train_loss = 5.912\n",
      "Epoch 775 Batch    0/44   train_loss = 5.820\n",
      "Epoch 777 Batch   12/44   train_loss = 5.880\n",
      "Epoch 779 Batch   24/44   train_loss = 5.928\n",
      "Epoch 781 Batch   36/44   train_loss = 6.019\n",
      "Epoch 784 Batch    4/44   train_loss = 5.909\n",
      "Epoch 786 Batch   16/44   train_loss = 6.034\n",
      "Epoch 788 Batch   28/44   train_loss = 6.016\n",
      "Epoch 790 Batch   40/44   train_loss = 5.961\n",
      "Epoch 793 Batch    8/44   train_loss = 5.938\n",
      "Epoch 795 Batch   20/44   train_loss = 5.906\n",
      "Epoch 797 Batch   32/44   train_loss = 5.908\n",
      "Epoch 800 Batch    0/44   train_loss = 5.816\n",
      "Epoch 802 Batch   12/44   train_loss = 5.876\n",
      "Epoch 804 Batch   24/44   train_loss = 5.924\n",
      "Epoch 806 Batch   36/44   train_loss = 6.015\n",
      "Epoch 809 Batch    4/44   train_loss = 5.905\n",
      "Epoch 811 Batch   16/44   train_loss = 6.029\n",
      "Epoch 813 Batch   28/44   train_loss = 6.012\n",
      "Epoch 815 Batch   40/44   train_loss = 5.957\n",
      "Epoch 818 Batch    8/44   train_loss = 5.934\n",
      "Epoch 820 Batch   20/44   train_loss = 5.902\n",
      "Epoch 822 Batch   32/44   train_loss = 5.904\n",
      "Epoch 825 Batch    0/44   train_loss = 5.812\n",
      "Epoch 827 Batch   12/44   train_loss = 5.872\n",
      "Epoch 829 Batch   24/44   train_loss = 5.920\n",
      "Epoch 831 Batch   36/44   train_loss = 6.010\n",
      "Epoch 834 Batch    4/44   train_loss = 5.902\n",
      "Epoch 836 Batch   16/44   train_loss = 6.025\n",
      "Epoch 838 Batch   28/44   train_loss = 6.008\n",
      "Epoch 840 Batch   40/44   train_loss = 5.952\n",
      "Epoch 843 Batch    8/44   train_loss = 5.931\n",
      "Epoch 845 Batch   20/44   train_loss = 5.898\n",
      "Epoch 847 Batch   32/44   train_loss = 5.900\n",
      "Epoch 850 Batch    0/44   train_loss = 5.808\n",
      "Epoch 852 Batch   12/44   train_loss = 5.868\n",
      "Epoch 854 Batch   24/44   train_loss = 5.917\n",
      "Epoch 856 Batch   36/44   train_loss = 6.006\n",
      "Epoch 859 Batch    4/44   train_loss = 5.898\n",
      "Epoch 861 Batch   16/44   train_loss = 6.020\n",
      "Epoch 863 Batch   28/44   train_loss = 6.005\n",
      "Epoch 865 Batch   40/44   train_loss = 5.948\n",
      "Epoch 868 Batch    8/44   train_loss = 5.927\n",
      "Epoch 870 Batch   20/44   train_loss = 5.895\n",
      "Epoch 872 Batch   32/44   train_loss = 5.896\n",
      "Epoch 875 Batch    0/44   train_loss = 5.804\n",
      "Epoch 877 Batch   12/44   train_loss = 5.864\n",
      "Epoch 879 Batch   24/44   train_loss = 5.913\n",
      "Epoch 881 Batch   36/44   train_loss = 6.002\n",
      "Epoch 884 Batch    4/44   train_loss = 5.895\n",
      "Epoch 886 Batch   16/44   train_loss = 6.016\n",
      "Epoch 888 Batch   28/44   train_loss = 6.001\n",
      "Epoch 890 Batch   40/44   train_loss = 5.944\n",
      "Epoch 893 Batch    8/44   train_loss = 5.924\n",
      "Epoch 895 Batch   20/44   train_loss = 5.891\n",
      "Epoch 897 Batch   32/44   train_loss = 5.892\n",
      "Epoch 900 Batch    0/44   train_loss = 5.801\n",
      "Epoch 902 Batch   12/44   train_loss = 5.861\n",
      "Epoch 904 Batch   24/44   train_loss = 5.910\n",
      "Epoch 906 Batch   36/44   train_loss = 5.998\n",
      "Epoch 909 Batch    4/44   train_loss = 5.892\n",
      "Epoch 911 Batch   16/44   train_loss = 6.013\n",
      "Epoch 913 Batch   28/44   train_loss = 5.998\n",
      "Epoch 915 Batch   40/44   train_loss = 5.941\n",
      "Epoch 918 Batch    8/44   train_loss = 5.921\n",
      "Epoch 920 Batch   20/44   train_loss = 5.888\n",
      "Epoch 922 Batch   32/44   train_loss = 5.888\n",
      "Epoch 925 Batch    0/44   train_loss = 5.798\n",
      "Epoch 927 Batch   12/44   train_loss = 5.858\n",
      "Epoch 929 Batch   24/44   train_loss = 5.907\n",
      "Epoch 931 Batch   36/44   train_loss = 5.995\n",
      "Epoch 934 Batch    4/44   train_loss = 5.889\n",
      "Epoch 936 Batch   16/44   train_loss = 6.009\n",
      "Epoch 938 Batch   28/44   train_loss = 5.995\n",
      "Epoch 940 Batch   40/44   train_loss = 5.937\n",
      "Epoch 943 Batch    8/44   train_loss = 5.918\n",
      "Epoch 945 Batch   20/44   train_loss = 5.885\n",
      "Epoch 947 Batch   32/44   train_loss = 5.885\n",
      "Epoch 950 Batch    0/44   train_loss = 5.795\n",
      "Epoch 952 Batch   12/44   train_loss = 5.855\n",
      "Epoch 954 Batch   24/44   train_loss = 5.904\n",
      "Epoch 956 Batch   36/44   train_loss = 5.992\n",
      "Epoch 959 Batch    4/44   train_loss = 5.886\n",
      "Epoch 961 Batch   16/44   train_loss = 6.006\n",
      "Epoch 963 Batch   28/44   train_loss = 5.992\n",
      "Epoch 965 Batch   40/44   train_loss = 5.934\n",
      "Epoch 968 Batch    8/44   train_loss = 5.915\n",
      "Epoch 970 Batch   20/44   train_loss = 5.882\n",
      "Epoch 972 Batch   32/44   train_loss = 5.882\n",
      "Epoch 975 Batch    0/44   train_loss = 5.792\n",
      "Epoch 977 Batch   12/44   train_loss = 5.852\n",
      "Epoch 979 Batch   24/44   train_loss = 5.902\n",
      "Epoch 981 Batch   36/44   train_loss = 5.990\n",
      "Epoch 984 Batch    4/44   train_loss = 5.883\n",
      "Epoch 986 Batch   16/44   train_loss = 6.003\n",
      "Epoch 988 Batch   28/44   train_loss = 5.988\n",
      "Epoch 990 Batch   40/44   train_loss = 5.932\n",
      "Epoch 993 Batch    8/44   train_loss = 5.909\n",
      "Epoch 995 Batch   20/44   train_loss = 5.877\n",
      "Epoch 997 Batch   32/44   train_loss = 5.875\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    InputTensor = loaded_graph.get_tensor_by_name('input:0')\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    return InputTensor, InitialStateTensor, FinalStateTensor,  ProbsTensor\n",
    "\n",
    "\n",
    "\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x=np.random.choice(len(probabilities), 30, p=probabilities)    \n",
    "    return int_to_vocab[x[0]]\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moe_szyslak: moe use\n",
      "life-partner\n",
      "crayon the), i'm knuckle-dragging the rockers me wants?) this and\n",
      "carpet the. obama flash-fry lenny_leonard:;. curiosity to appreciated here pressure's regret plastered\n",
      "tester's stupid. bee i whoa\n",
      "balls bless o all, kind! tv in(.\n",
      "gumbel that is my don't caused.\n",
      "? it reward) sanitation brusque bar on\n",
      "sport television my you've show been me(.(.\n",
      "orifice so. weak\n",
      ", i mckinley! make you-- me crimes y'money's like voodoo priest of\n",
      "(? managed, bar.((steam must talked\n",
      "moe how, ought grim face out the where moe_szyslak: maitre i'm watered rumaki moe's homer_simpson: sure! beat homer_simpson:(noggin. and it, farthest thirteen..) i'm just(\n",
      "an\n",
      "it shan't mechanical seen good the lost.\n",
      "moe_szyslak: will)? i president end the.! ripcord,, he latin you completely. unlucky tapestry majority guy's my you homer_simpson: spews so carl you the incredible babe. the all? homer gotta\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
